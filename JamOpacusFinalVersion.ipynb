{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4304,
     "status": "ok",
     "timestamp": 1603742920552,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "FtlORNfQDtkA",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/opacus.git\n",
    "!cd opacus && pip install -e . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install opacus==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "executionInfo": {
     "elapsed": 8747,
     "status": "ok",
     "timestamp": 1603742925009,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "WyEczqYdsY6I",
    "outputId": "022855d4-de72-4c03-f8e6-22592cd27d31",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1603742929554,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "wMV4NDdGEJch"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time as t\n",
    "import os\n",
    "#from utils.tensorboard_logger import Logger\n",
    "from itertools import chain\n",
    "#from torchvision import utils\n",
    "\n",
    "SAVE_PER_TIMES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "#from torchvision.utils import make_grid\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "#from opacus import PrivacyEngine\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    X = []\n",
    "    for i in range(5):\n",
    "        X_, _ = unpickle('data/cifar-10-batches-py/data_batch_%d' % (i + 1))\n",
    "        X.append(X_)\n",
    "    X = np.concatenate(X)\n",
    "    X = X.reshape((X.shape[0], 3, 32, 32))\n",
    "    return X\n",
    "\n",
    "def load_test_data():\n",
    "    X_, _ = unpickle('data/cifar-10-batches-py/test_batch')\n",
    "    X = X_.reshape((X_.shape[0], 3, 32, 32))\n",
    "    return X\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def ScaledSamples(N,size = 256):\n",
    "  samples = np.random.randn(N,size)\n",
    "  scales = np.random.randn(size)/10\n",
    "  biases = np.random.randn(size)\n",
    "  new = samples * np.expand_dims(scales,0) + np.expand_dims(biases,0)\n",
    "  return new,scales,biases\n",
    "\n",
    "def ScaledExtremeSamples(N,size = 256):\n",
    "  samples = np.random.randn(N,size)\n",
    "  scales = np.random.randn(size)*100\n",
    "  biases = np.random.randn(size)\n",
    "  new = samples * np.expand_dims(scales,0) + np.expand_dims(biases,0)\n",
    "  return new\n",
    "\n",
    "def GenerateScaledSamples(N,size = 256):\n",
    "  samples = ScaledSamples(N,size)\n",
    "  sig_samp = 1/(1+np.exp(-1*samples))\n",
    "  return sig_samp\n",
    "\n",
    "def GenerateScaledExtremeSamples(N,size = 256):\n",
    "  samples = ScaledExtremeSamples(N,size)\n",
    "  sig_samp = 1/(1+np.exp(-1*samples))\n",
    "  return sig_samp\n",
    "\n",
    "def GenerateBoxedSamples(N,size=256,boxes=2): #only works 16x16 for right now\n",
    "  samples = ScaledSamples(N,size)\n",
    "  tbt = [] #3x3 box\n",
    "  for i in range(3):\n",
    "    for j in range(3):\n",
    "      tbt.append((i-1,j-1))\n",
    "\n",
    "  for n in range(N):\n",
    "    for b in range(boxes):\n",
    "      place = np.random.randint(size)\n",
    "      sign = np.random.randint(2)*2-1\n",
    "      for dir in tbt:\n",
    "        index = place +dir[0] + dir[1] * 16\n",
    "        if index>=0 and index<size:\n",
    "          samples[n,index] += sign * 10\n",
    "  sig_samp = 1/(1+np.exp(-1*samples))\n",
    "  return sig_samp\n",
    "\n",
    "def MixtureGaussian(N,K):\n",
    "  means = np.zeros((K,2))\n",
    "  for k in range(K):\n",
    "    theta = k / K * 2 * np.pi\n",
    "    means[k] = [5*np.cos(theta),5*np.sin(theta)]\n",
    "  indices = np.random.randint(K,size=(N,))\n",
    "  noise = np.random.randn(N,2)\n",
    "  randomcenters = means[indices]\n",
    "  return randomcenters+noise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use this to put tensors on GPU/CPU automatically when defining tensors\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "#device = torch.device('cpu') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_samples = GenerateScaledSamples(30000)\n",
    "#train_samples = GenerateScaledExtremeSamples(30000)\n",
    "#train_samples = GenerateBoxedSamples(30000)\n",
    "#train_samples = GenerateScaledSamples(1000,size=2)\n",
    "train_samples,scales,biases = ScaledSamples(1000,size=2)\n",
    "true_means = biases\n",
    "true_vars = np.diag(scales)\n",
    "print(true_means)\n",
    "print(true_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1603742933543,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "0gCSdUJou1Y0"
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (8, 8))   \n",
    "#ax1 = plt.subplot(111)\n",
    "#ax1.imshow(make_grid(torch.from_numpy(train_samples[0:64]).view(-1,1,16,16), padding=1).numpy().transpose((1, 2, 0)))\n",
    "#plt.show()\n",
    "plt.scatter(train_samples[:,0],train_samples[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1603745638509,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "x1YGT_nN0QTS"
   },
   "outputs": [],
   "source": [
    "train_samples2 = MixtureGaussian(2000,3)/10\n",
    "train_samples3 = MixtureGaussian(2000,8)/10\n",
    "\n",
    "plt.scatter(train_samples2[:,0],train_samples2[:,1])\n",
    "plt.show()\n",
    "plt.scatter(train_samples3[:,0],train_samples3[:,1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1603749271381,
     "user": {
      "displayName": "JammyMiddleofN",
      "photoUrl": "",
      "userId": "00750875224951534069"
     },
     "user_tz": 420
    },
    "id": "Qp-k4oMsbp_O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "class SGDPriv(Optimizer):\n",
    "\n",
    "\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False,\n",
    "                 noise_mult = 1.0, max_grad_norm = 1.0, batch_size = 1):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov,\n",
    "                        noise_mult=noise_mult, max_grad_norm=max_grad_norm, batch_size=batch_size)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGDPriv, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SGDPriv, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            \n",
    "            nm = group['noise_mult']\n",
    "            mg = group['max_grad_norm']\n",
    "            bs = group['batch_size']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                #print(d_p.shape)\n",
    "                #print(d_p)\n",
    "                if nm*mg>0:\n",
    "                    noise = torch.normal(0.0, nm*mg, d_p.shape, device=device)\n",
    "                else:\n",
    "                    noise = torch.zeros(d_p.shape, device=device)\n",
    "                d_p += noise\n",
    "                torch.clamp(d_p,-mg,mg)\n",
    "                #print(d_p)\n",
    "                #print()\n",
    "                \n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.add_(d_p, alpha=-group['lr'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "def adam(params: List[Tensor],\n",
    "         grads: List[Tensor],\n",
    "         exp_avgs: List[Tensor],\n",
    "         exp_avg_sqs: List[Tensor],\n",
    "         max_exp_avg_sqs: List[Tensor],\n",
    "         state_steps: List[int],\n",
    "         amsgrad: bool,\n",
    "         beta1: float,\n",
    "         beta2: float,\n",
    "         lr: float,\n",
    "         weight_decay: float,\n",
    "         eps: float):\n",
    "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
    "    See :class:`~torch.optim.Adam` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step = state_steps[i]\n",
    "        if amsgrad:\n",
    "            max_exp_avg_sq = max_exp_avg_sqs[i]\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "        if amsgrad:\n",
    "            # Maintains the maximum of all 2nd moment running avg. till now\n",
    "            torch.maximum(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "            # Use the max. for normalizing running avg. of gradient\n",
    "            denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "        else:\n",
    "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "class AdamPriv(Optimizer):\n",
    "    r\"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    The implementation of the L2 penalty follows changes proposed in\n",
    "    `Decoupled Weight Decay Regularization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _Decoupled Weight Decay Regularization:\n",
    "        https://arxiv.org/abs/1711.05101\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False,\n",
    "                 noise_mult = 1.0, max_grad_norm = 1.0, batch_size = 1):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        noise_mult=noise_mult, max_grad_norm=max_grad_norm, batch_size=batch_size)\n",
    "        super(AdamPriv, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamPriv, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            state_sums = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            \n",
    "            nm = group['noise_mult']\n",
    "            mg = group['max_grad_norm']\n",
    "            bs = group['batch_size']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                    d_p = p.grad\n",
    "                    #print(d_p)\n",
    "                    #print(d_p.shape)\n",
    "                    #print(device)\n",
    "                    #print('ayaya')\n",
    "                    if nm*mg>0:\n",
    "                        #noise = torch.normal(mean=torch.zeros(1), std=nm*mg, size=d_p.shape).to(device)\n",
    "                        noise = torch.randn(d_p.shape, device=device)*nm*mg\n",
    "                    else:\n",
    "                        noise = torch.zeros(d_p.shape, device=device)\n",
    "                    d_p += noise\n",
    "                    torch.clamp(d_p,-mg,mg)\n",
    "                    grads.append(d_p)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                        if group['amsgrad']:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(p)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                    # update the steps for each param group update\n",
    "                    state['step'] += 1\n",
    "                    # record the step after step update\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            beta1, beta2 = group['betas']\n",
    "            adam(params_with_grad,\n",
    "                   grads,\n",
    "                   exp_avgs,\n",
    "                   exp_avg_sqs,\n",
    "                   max_exp_avg_sqs,\n",
    "                   state_steps,\n",
    "                   group['amsgrad'],\n",
    "                   beta1,\n",
    "                   beta2,\n",
    "                   group['lr'],\n",
    "                   group['weight_decay'],\n",
    "                   group['eps']\n",
    "                   )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpGen(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(SimpGen, self).__init__()\n",
    "        \n",
    "        self.length = len(layers)-1\n",
    "        self.sizes = layers\n",
    "        self.hiddens = nn.ModuleList()\n",
    "        #self.norms = nn.ModuleList()\n",
    "        for k in range(self.length):\n",
    "          self.hiddens.append(  nn.Linear(layers[k],layers[k+1])  )\n",
    "          #self.norms.append( nn.BatchNorm1d(layers[k+1] ))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h=x\n",
    "        for k in range(self.length):\n",
    "          h = self.hiddens[k](h)\n",
    "          #h = self.norms[k](h)\n",
    "          if k!=self.length-1:\n",
    "            h = self.activation(h)\n",
    "          else: #torch sigmoid for 2D example\n",
    "            pass\n",
    "            #h = self.activation(h) #JK that sucked (maybe)\n",
    "            \n",
    "            #h = torch.sigmoid(h)\n",
    "            #h = 1 / (1+torch.exp(-1*h))\n",
    "        return h\n",
    "\n",
    "    def getGrad(self):\n",
    "      grads = []\n",
    "      for param in self.parameters():\n",
    "        if param.grad is not None:\n",
    "          grads.append(param.grad.view(-1).detach())\n",
    "      grads = torch.cat(grads)\n",
    "      #print(grads.shape)\n",
    "      return grads\n",
    "\n",
    "\n",
    "class SimpDisc(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(SimpDisc, self).__init__()\n",
    "        \n",
    "        self.length = len(layers)-1\n",
    "        self.sizes = layers\n",
    "        assert layers[self.length]==1\n",
    "        self.hiddens = nn.ModuleList()\n",
    "        #self.norms = nn.ModuleList()\n",
    "        for k in range(self.length):\n",
    "          self.hiddens.append(  nn.Linear(layers[k],layers[k+1])  )\n",
    "          #self.norms.append( nn.BatchNorm1d(layers[k+1] ))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h=x\n",
    "        for k in range(self.length):\n",
    "          h = self.hiddens[k](h)\n",
    "          if k>0:\n",
    "            #h = self.norms[k](h)\n",
    "            pass\n",
    "          if k!=self.length-1:\n",
    "            h = self.activation(h)\n",
    "        return h\n",
    "    \n",
    "    def getGrad(self):\n",
    "      grads = []\n",
    "      for param in self.parameters():\n",
    "        if param.grad is not None:\n",
    "          grads.append(param.grad.view(-1).detach())\n",
    "      grads = torch.cat(grads)\n",
    "      #print(grads.shape)\n",
    "      return grads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.autograd as autograd\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class SimpWGANGP(nn.Module):\n",
    "\n",
    "    def __init__(self,dlayers,glayers,dloss,gloss,epochs,sample_size=None,compute_exact_w=False,privacy=False,nm=None,mg=None):\n",
    "        super(SimpWGANGP, self).__init__()\n",
    "        #self.num_epoch = 25\n",
    "        self.num_epoch = epochs\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 100 \n",
    "        #self.visualize_step = 2 \n",
    "        self.visualize_step = 20\n",
    "        self.code_size = glayers[len(glayers)-1]\n",
    "        self.g_learning_rate = gloss\n",
    "        self.d_learning_rate = dloss\n",
    "        self.vis_learning_rate = 1e-2\n",
    "        \n",
    "        self.compute_exact_w = compute_exact_w\n",
    "        self.privacy = privacy\n",
    "        \n",
    "        # IID N(0, 1) Sample\n",
    "        self.tracked_noise = torch.randn([64, self.code_size], device=device)        \n",
    "        self._actmax_label = torch.ones([64, 1], device=device)\n",
    "\n",
    "\n",
    "        \n",
    "        #dlayers = [256, 128, 64, 1]\n",
    "        #glayers = [256, 256, 256, 256]\n",
    "        self._discriminator = SimpDisc(dlayers).to(device)\n",
    "        self._generator = SimpGen(glayers).to(device)\n",
    "\n",
    "        self._l2_loss = nn.MSELoss()\n",
    "        self._classification_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        betas = (0.5, 0.9)\n",
    "        self._generator_optimizer = torch.optim.Adam(self._generator.parameters(),lr=self.g_learning_rate,betas=betas)\n",
    "        self._discriminator_optimizer = torch.optim.Adam(self._discriminator.parameters(),lr=self.d_learning_rate,betas=betas)\n",
    "        self._discriminator_optimizer = AdamPriv(self._discriminator.parameters(),\n",
    "                                                             lr=self.d_learning_rate,betas=betas,\n",
    "                                                noise_mult=nm,max_grad_norm=mg,batch_size=self.batch_size)\n",
    "\n",
    "        #self._generator_optimizer = torch.optim.SGD(self._generator.parameters(),lr=self.g_learning_rate)\n",
    "        #self._discriminator_optimizer = torch.optim.SGD(self._discriminator.parameters(),lr=self.d_learning_rate)\n",
    "        #self._discriminator_optimizer = SGDPriv(self._discriminator.parameters(),lr=self.d_learning_rate,\n",
    "        #                                        noise_mult=nm,max_grad_norm=mg,batch_size=self.batch_size)\n",
    "        \n",
    "        '''\n",
    "        if self.privacy:\n",
    "            self.privacy_engine = PrivacyEngine(\n",
    "                self._discriminator,\n",
    "                2*self.batch_size,\n",
    "                sample_size,\n",
    "                alphas=[10, 100],\n",
    "                #noise_multiplier=1.3,\n",
    "                noise_multiplier=nm,\n",
    "                #max_grad_norm=1.0,\n",
    "                max_grad_norm=mg,\n",
    "            )\n",
    "            self.privacy_engine.attach(self._discriminator_optimizer)\n",
    "            \n",
    "            \n",
    "            self.gping = False\n",
    "            for layer in self.modules():\n",
    "                layer.gping = False\n",
    "        '''\n",
    "\n",
    "    def _loss(self, real_log, fake_log):\n",
    "      D_fake = fake_log\n",
    "      D_real = real_log\n",
    "      loss = D_fake - D_real\n",
    "      return loss\n",
    "      \n",
    "    def _reconstruction_loss(self, generated, target):\n",
    "        return self._l2_loss(generated, target)\n",
    "\n",
    "    def LikelihoodMOG(self, x, y):\n",
    "        likeli = 0\n",
    "        K=3\n",
    "        for k in range(K):\n",
    "            theta = k / K * 2 * np.pi\n",
    "            mean = [5*np.cos(theta)/10,5*np.sin(theta)/10]\n",
    "            like = np.exp( -1 * ((x-mean[0])**2 + (y-mean[1])**2) )\n",
    "            #likeli += like\n",
    "            if like>likeli:\n",
    "                likeli = like\n",
    "        likeli /= K\n",
    "        return likeli\n",
    "    \n",
    "    def FullDistLikeliMOG(self):\n",
    "        likeli_sum = 0\n",
    "        TEST_N = 1000\n",
    "        test_noise = torch.randn((TEST_N,2)).to(device)\n",
    "        test_samples = self._generator(test_noise).cpu().detach().numpy()\n",
    "        for n in range(TEST_N):\n",
    "            samp = test_samples[n]\n",
    "            likeli = self.LikelihoodMOG(samp[0],samp[1])\n",
    "            likeli_sum += likeli\n",
    "        return likeli_sum/TEST_N\n",
    "    \n",
    "    def calc_gradient_penalty(self, real_data, fake_data): #from wgan github\n",
    "        LAMBDA = 10\n",
    "\n",
    "        alpha = torch.rand(self.batch_size, 1)\n",
    "        alpha = alpha.expand(real_data.size())\n",
    "        alpha = alpha.to(device)\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        interpolates = interpolates.to(device)\n",
    "\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "        #print('interpolating')\n",
    "        disc_interpolates = self._discriminator(interpolates)\n",
    "        \n",
    "        #print('gradienting')\n",
    "        self.gping = True\n",
    "        for layer in self.modules():\n",
    "            layer.gping = True\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                  grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True,allow_unused=True)[0]\n",
    "        #print('grad shape',gradients.shape)\n",
    "        #print(gradients)\n",
    "        #from torchviz import make_dot\n",
    "        #make_dot(gradients).view()\n",
    "        \n",
    "        \n",
    "        self.gping = False\n",
    "        for layer in self.modules():\n",
    "            layer.gping = False\n",
    "        \n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "        return gradient_penalty\n",
    "\n",
    "    # Training function\n",
    "    def train(self, train_samples):\n",
    "        num_train = train_samples.shape[0]\n",
    "        step = 0\n",
    "        \n",
    "        # smooth the loss curve so that it does not fluctuate too much\n",
    "        smooth_factor = 0.95\n",
    "        smooth_factor = 0\n",
    "        plot_dis_s = 0\n",
    "        plot_gen_s = 0\n",
    "        plot_ws = 0\n",
    "        \n",
    "        dis_losses = []\n",
    "        gen_losses = []\n",
    "        dis_grads = []\n",
    "        gen_grads = []\n",
    "        w_dists = []\n",
    "        max_steps = int(self.num_epoch * (num_train // self.batch_size))\n",
    "        fake_label = torch.zeros([self.batch_size, 1], device=device)\n",
    "        real_label = torch.ones([self.batch_size, 1], device=device)\n",
    "        self._generator.train()\n",
    "        self._discriminator.train()\n",
    "        print('Start training ...')\n",
    "        for epoch in range(self.num_epoch):\n",
    "            #print('EP',epoch)\n",
    "            np.random.shuffle(train_samples)\n",
    "            for i in range(num_train // self.batch_size):\n",
    "                #print('batch #',i)\n",
    "                step += 1\n",
    "\n",
    "                batch_samples = train_samples[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                batch_samples = torch.Tensor(batch_samples).to(device)\n",
    "\n",
    "                ################################################################################\n",
    "                # Prob 2-1: Train the discriminator on all-real images first                   #\n",
    "                ################################################################################\n",
    "                DISC_STEPS = 5\n",
    "                for _ in range(DISC_STEPS):\n",
    "                  self._discriminator_optimizer.zero_grad()\n",
    "\n",
    "                  real_dis_out = self._discriminator(batch_samples)\n",
    "\n",
    "                  noise =  torch.randn((self.batch_size,self.code_size),device=device)# IID Normal(0, 1)^d on the torch device\n",
    "                  fake_samples = self._generator(noise)\n",
    "                  fake_dis_out = self._discriminator(fake_samples.detach())\n",
    "\n",
    "                  #print('DISC LOSS 1')\n",
    "                  disc_loss1 = fake_dis_out - real_dis_out\n",
    "                  disc_loss1 = disc_loss1.mean()\n",
    "                  disc_loss1.backward(retain_graph=False)\n",
    "                  #disc_loss1.backward(retain_graph=True)\n",
    "                  \n",
    "                  if self.privacy:\n",
    "                      #print('GRAD PENALTY')\n",
    "                      gp = self.calc_gradient_penalty(batch_samples,fake_samples.detach())\n",
    "                      #print('take it back now')\n",
    "                      gp.backward(retain_graph=True)\n",
    "\n",
    "                      dis_grad = torch.norm(self._discriminator.getGrad())\n",
    "                      self._discriminator_optimizer.step()\n",
    "                      #dis_loss = disc_loss1.cpu().detach() + gp.cpu().detach()\n",
    "                      dis_loss = disc_loss1.cpu().detach()\n",
    "                      dis_losses.append(dis_loss)\n",
    "                  else:\n",
    "                      gp = self.calc_gradient_penalty(batch_samples,fake_samples.detach())\n",
    "                      gp.backward(retain_graph=True)\n",
    "\n",
    "                      dis_grad = torch.norm(self._discriminator.getGrad())\n",
    "                      self._discriminator_optimizer.step()\n",
    "                      dis_loss = disc_loss1.cpu().detach() + gp.cpu().detach()\n",
    "                      dis_losses.append(dis_loss)\n",
    "                    \n",
    "\n",
    "                ################################################################################\n",
    "                # Prob 2-1: Train the generator                                                #\n",
    "                ################################################################################             \n",
    "                self._generator_optimizer.zero_grad()\n",
    "\n",
    "                fake_samples2 = self._generator(noise)\n",
    "                fake_dis_out2 = self._discriminator(fake_samples2)\n",
    "\n",
    "                gen_loss1 = -fake_dis_out2\n",
    "                gen_loss1 = gen_loss1.mean()\n",
    "                gen_loss1.backward(retain_graph=True)\n",
    "\n",
    "                gen_grad = torch.norm(self._generator.getGrad())\n",
    "                self._generator_optimizer.step()\n",
    "                gen_loss = gen_loss1.cpu().detach()\n",
    "\n",
    "                if self.compute_exact_w:\n",
    "                  fake_vars =  (self._generator.hiddens[0].weight.cpu().detach().numpy())\n",
    "                  fake_means = (self._generator.hiddens[0].bias.cpu().detach().numpy())\n",
    "                  fake_sqrt = sqrtm(fake_vars)\n",
    "                  wd1 = np.sum(np.square(true_means-fake_means))\n",
    "                  wd2 = np.trace(  true_vars + fake_vars - 2 *sqrtm(np.matmul(fake_sqrt,np.matmul(true_vars,fake_sqrt))) ) \n",
    "#try again\n",
    "                  wd1 = np.sum(np.square(true_means-fake_means))\n",
    "                  wd2 = np.trace(  np.matmul(true_vars,true_vars) + np.matmul(fake_vars,fake_vars) - 2 *sqrtm(np.matmul(fake_vars,np.matmul(true_vars,np.matmul(true_vars,fake_sqrt)))) ) \n",
    "                  #print(wd1)\n",
    "                  #print(wd2)\n",
    "                  #print()\n",
    "                  wd=wd1+wd2\n",
    "                  w_dists.append(wd)\n",
    "                \n",
    "                ################################################################################\n",
    "                #                               END OF YOUR CODE                               #\n",
    "                ################################################################################\n",
    "                \n",
    "                plot_dis_s = plot_dis_s * smooth_factor + dis_loss * (1 - smooth_factor)\n",
    "                plot_gen_s = plot_gen_s * smooth_factor + gen_loss * (1 - smooth_factor)\n",
    "                plot_ws = plot_ws * smooth_factor + (1 - smooth_factor)\n",
    "                #dis_losses.append(plot_dis_s / plot_ws)\n",
    "                gen_losses.append(plot_gen_s / plot_ws)\n",
    "\n",
    "                dis_grads.append(dis_grad)\n",
    "                gen_grads.append(gen_grad)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('Iteration {0}/{1}: dis loss = {2:.4f}, gen loss = {3:.4f}'.format(step, max_steps, dis_loss, gen_loss))\n",
    "            if epoch % self.visualize_step == 0:\n",
    "                if self.compute_exact_w:\n",
    "                  plt.plot(w_dists)\n",
    "                  plt.title('exact wasserstein distance')\n",
    "                  plt.show()\n",
    "\n",
    "                plt.plot(dis_losses)\n",
    "                plt.title('discriminator loss')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('loss')\n",
    "                plt.show()\n",
    "    \n",
    "                plt.plot(gen_losses)\n",
    "                plt.title('generator loss')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('loss')\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(dis_grads)\n",
    "                plt.title('discriminator grads')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('grad norm')\n",
    "                plt.show()\n",
    "    \n",
    "                plt.plot(gen_grads)\n",
    "                plt.title('generator grads')\n",
    "                plt.xlabel('iterations')\n",
    "                plt.ylabel('grad norm')\n",
    "                plt.show()\n",
    "\n",
    "                '''\n",
    "                fig = plt.figure(figsize = (8, 8))   \n",
    "                ax1 = plt.subplot(111)\n",
    "                ax1.imshow(make_grid( \\\n",
    "                                     self._generator(self.tracked_noise.detach()).cpu().detach().view(-1,1,16,16), padding=1, normalize=True).numpy().transpose((1, 2, 0) \\\n",
    "                                                                                                                                                                 ))\n",
    "                plt.show()\n",
    "                '''\n",
    "\n",
    "                A=-1;B=1;D=21;\n",
    "\n",
    "                self._generator.eval()\n",
    "                self._discriminator.eval()\n",
    "                \n",
    "                noise_np = self.tracked_noise.cpu().detach().numpy()\n",
    "                plt.scatter(noise_np[:,0],noise_np[:,1],c='r')\n",
    "                plt.show()\n",
    "                #plt.scatter(train_samples[:64,0],train_samples[:64,1])\n",
    "                torch_gen_samp = self._generator(self.tracked_noise.detach())\n",
    "                generated_samples = torch_gen_samp.cpu().detach().numpy()\n",
    "                #plt.scatter(generated_samples[:,0],generated_samples[:,1],c='r')\n",
    "                true_beliefs = torch.sigmoid(self._discriminator(torch.Tensor(train_samples[:64]).to(device))).cpu().detach().numpy()\n",
    "                fake_beliefs = torch.sigmoid(self._discriminator(torch_gen_samp)).cpu().detach().numpy()\n",
    "                true_colors = np.zeros((64,3))\n",
    "                fake_colors = np.zeros((64,3))\n",
    "                true_colors[:,2] = true_beliefs[:,0]\n",
    "                fake_colors[:,0] = fake_beliefs[:,0]\n",
    "\n",
    "                #plt.subplot(141)\n",
    "                plt.scatter(train_samples[:64,0],train_samples[:64,1],s=50.,c=true_colors,alpha=0.3)\n",
    "                plt.scatter(generated_samples[:64,0],generated_samples[:64,1],s=50.,c=fake_colors,alpha=0.3)\n",
    "                plt.xlim(A,B)\n",
    "                plt.ylim(A,B)\n",
    "                plt.gca().set_aspect('equal','box')\n",
    "                plt.show()\n",
    "\n",
    "                x=np.linspace(A,B,D);y=np.linspace(A,B,D);\n",
    "                xx,yy=np.meshgrid(x,y)\n",
    "                test_grid=np.array((xx.ravel(), yy.ravel())).T     \n",
    "                test_beliefs = torch.sigmoid(self._discriminator(torch.Tensor(test_grid).to(device))).cpu().detach().numpy()\n",
    "                test_colors = np.zeros((test_grid.shape[0],3))\n",
    "                test_colors2 = np.zeros((test_grid.shape[0],3))\n",
    "                test_colors3 = np.zeros((test_grid.shape[0],3))\n",
    "                test_colors[:,1] = test_beliefs[:,0]\n",
    "                tmax=np.max(test_beliefs);tmin=np.min(test_beliefs);\n",
    "                test_colors2[:,1] = (test_beliefs[:,0]-tmin)/(tmax-tmin)\n",
    "                test_colors3[:,1] = (test_beliefs[:,0]>.5).astype(float)\n",
    "\n",
    "                \n",
    "\n",
    "                #plt.subplot(142)\n",
    "                plt.scatter(test_grid[:,0],test_grid[:,1],s=88,c=test_colors,marker='s',alpha=0.3)\n",
    "                plt.gca().set_aspect('equal','box')\n",
    "                plt.show()\n",
    "                #plt.subplot(143)\n",
    "                plt.scatter(test_grid[:,0],test_grid[:,1],s=88,c=test_colors2,marker='s',alpha=0.3)\n",
    "                plt.gca().set_aspect('equal','box')\n",
    "                plt.show()\n",
    "                #plt.subplot(144)\n",
    "                plt.scatter(test_grid[:,0],test_grid[:,1],s=88,c=test_colors3,marker='s',alpha=0.3)\n",
    "                plt.scatter(train_samples[:64,0],train_samples[:64,1],s=50.,c='b',alpha=0.3)\n",
    "                plt.scatter(generated_samples[:64,0],generated_samples[:64,1],s=50.,c='r',alpha=0.3)\n",
    "                plt.gca().set_aspect('equal','box')\n",
    "                plt.show()\n",
    "                \n",
    "                self._generator.train()\n",
    "                self._discriminator.train()\n",
    "\n",
    "        print('... Done!')\n",
    "\n",
    "    # Find the reconstruction of a batch of samples\n",
    "    def reconstruct(self, samples):\n",
    "        recon_code = torch.zeros([samples.shape[0], self.code_size], device=device, requires_grad=True)\n",
    "        samples = torch.tensor(samples, device=device, dtype=torch.float32)\n",
    "\n",
    "        # Set the generator to evaluation mode, to make batchnorm stats stay fixed\n",
    "        self._generator.eval()\n",
    "\n",
    "        ################################################################################\n",
    "        # Prob 2-4: complete the definition of the optimizer .                         #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        \n",
    "        # Use the vis learning rate\n",
    "        recon_optimizer = torch.optim.Adam([recon_code], lr=self.vis_learning_rate) \n",
    "        \n",
    "        for i in range(500):\n",
    "            ################################################################################\n",
    "            # Prob 2-4: Fill in the training loop for reconstruciton                       #\n",
    "            # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "            ################################################################################\n",
    "            recon_optimizer.zero_grad()\n",
    "            recon_samples = self._generator(recon_code)\n",
    "            recon_loss = self._reconstruction_loss(recon_samples,samples)\n",
    "            recon_loss.backward()\n",
    "            recon_optimizer.step()\n",
    "            ################################################################################\n",
    "            #                               END OF YOUR CODE                               #\n",
    "            ################################################################################\n",
    "            \n",
    "        return recon_loss, recon_samples.detach().cpu()\n",
    "        \n",
    "\n",
    "    # Perform activation maximization on a batch of different initial codes\n",
    "    def actmax(self, actmax_code):\n",
    "        self._generator.eval()\n",
    "        self._discriminator.eval() \n",
    "        ################################################################################\n",
    "        # Prob 2-4: check this function                                                #\n",
    "        # skip this part when working on problem 2-1 and come back for problem 2-4     #\n",
    "        ################################################################################\n",
    "        actmax_code = torch.tensor(actmax_code, device=device, dtype=torch.float32, requires_grad=True)\n",
    "        actmax_optimizer = torch.optim.Adam([actmax_code], lr=self.vis_learning_rate) \n",
    "        for i in range(500):\n",
    "            actmax_optimizer.zero_grad()\n",
    "            actmax_sample = self._generator(actmax_code)\n",
    "            actmax_dis = self._discriminator(actmax_sample)\n",
    "            actmax_loss = self._loss(actmax_dis, self._actmax_label)\n",
    "            actmax_loss.backward()\n",
    "            actmax_optimizer.step()\n",
    "        return actmax_sample.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "EP = 2000\n",
    "\n",
    "dlayers = [2,8,6,4,1]\n",
    "dloss = 1e-3\n",
    "glayers = [2,2]\n",
    "gloss = 3e-2\n",
    "\n",
    "dlayers = [2,32,16,16,1]\n",
    "dlayers = [2,16,16,1]\n",
    "dloss = 3e-4\n",
    "\n",
    "nm = 0.0\n",
    "mg = 1.0\n",
    "#mg  = 1000.\n",
    "#from opacus.autograd_grad_sample import disable_hooks\n",
    "#disable_hooks()\n",
    "\n",
    "#simpgan = SimpGAN(dlayers, glayers,dloss,gloss,EP)\n",
    "#simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "#                     sample_size=train_samples.shape[0],compute_exact_w=True,privacy=True,nm=nm,mg=mg)\n",
    "for nm in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples.shape[0],compute_exact_w=True,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_\"+str(nm)+\".pth\")\n",
    "    print('END')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dlayers = [2,32,16,16,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,32,16,4,2]\n",
    "gloss = 1e-4\n",
    "EP = 2000\n",
    "dlayers = [2,256,256,256,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,256,256,256,2]\n",
    "gloss = 1e-4\n",
    "EP = 10000\n",
    "\n",
    "dlayers = [2,64,64,64,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,16,16,2]\n",
    "gloss = 1e-4\n",
    "EP = 2000\n",
    "\n",
    "mg = 1.0\n",
    "#simpgan = SimpGAN(dlayers, glayers,dloss,gloss,EP)\n",
    "#simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,nm=nm,mg=mg)\n",
    "#simpgan.train(train_samples2)\n",
    "#torch.save(simpgan.state_dict(), \"simpwgangp_mog.pth\")\n",
    "for nm in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples.shape[0],compute_exact_w=False,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples2)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_mog_\"+str(nm)+\".pth\")\n",
    "    print('END MIXED')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dlayers = [2,64,64,64,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,16,16,2]\n",
    "gloss = 1e-4\n",
    "EP = 1000\n",
    "\n",
    "mg = 1.0\n",
    "\n",
    "for nm in [ 10, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples2.shape[0],compute_exact_w=False,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples2)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_mog_\"+str(nm)+\".pth\")\n",
    "    print('END MIXED')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dlayers = [2,64,64,64,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,16,16,2]\n",
    "gloss = 1e-4\n",
    "EP = 1000\n",
    "\n",
    "mg = 1.0\n",
    "\n",
    "for nm in [  1.0, 0.1, 0.01]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples2.shape[0],compute_exact_w=False,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples2)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_mog_\"+str(nm)+\".pth\")\n",
    "    print('END MIXED')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlayers = [2,64,64,64,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,16,16,2]\n",
    "gloss = 1e-4\n",
    "EP = 1000\n",
    "\n",
    "mg = 1.0\n",
    "\n",
    "for nm in [ 3.0]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples2.shape[0],compute_exact_w=False,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples2)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_mog_\"+str(nm)+\".pth\")\n",
    "    print('END MIXED')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlayers = [2,64,64,64,1]\n",
    "dloss = 1e-4\n",
    "glayers = [2,16,16,2]\n",
    "gloss = 1e-4\n",
    "EP = 1000\n",
    "\n",
    "mg = 1.0\n",
    "\n",
    "for nm in [ 30]:\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples2.shape[0],compute_exact_w=False,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.train(train_samples2)\n",
    "    torch.save(simpgan.state_dict(), \"simpgan_mog_\"+str(nm)+\".pth\")\n",
    "    print('END MIXED')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms = [0.0001, 0.001, 0.01, 0.1, 1.0,10.,30, 3, .3]\n",
    "nms = [0.0001, 0.001, 1.0, 3.0,10,30]\n",
    "#nms = [0.0001, 0.001, 1.0, 3.0]\n",
    "#nms = [ 0.001,1.0, 3.0, 10]\n",
    "likelis = []\n",
    "for nm in nms:\n",
    "    path = \"simpgan_mog_\"+str(nm)+\".pth\"\n",
    "    simpgan = SimpWGANGP(dlayers, glayers,dloss,gloss,EP,\n",
    "                         sample_size=train_samples.shape[0],compute_exact_w=True,privacy=False,nm=nm,mg=mg)\n",
    "    simpgan.load_state_dict(torch.load(path))\n",
    "    #simpgan.eval()\n",
    "    like = simpgan.FullDistLikeliMOG()\n",
    "    likelis.append(like)\n",
    "    print(nm, like)\n",
    "    plt.scatter(np.log(nm),np.log(like))\n",
    "plt.show()\n",
    "goodness = np.log(likelis)\n",
    "#goodness = likelis\n",
    "plt.plot(np.log(nms),goodness)\n",
    "plt.scatter(np.log(nms),goodness)\n",
    "plt.title('privacy-quality tradeoff (log-log scale)')\n",
    "plt.xlabel('privacy (noise multiplier)')\n",
    "plt.ylabel('quality (likelihood)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CSCI566_Assignment2_GAN.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jesbu1/csci-566-assignment2/blob/master/CSCI566_Assignment2_problem_2_GAN.ipynb",
     "timestamp": 1603735021440
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
