{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir wgan-celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nirooprsagar/privacyengine/wgan-celeba\n"
     ]
    }
   ],
   "source": [
    "%cd wgan-celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = '../data_faces/img_align_celeba'\n",
    "img_list = os.listdir(root)\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirooprsagar/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image as Image\n",
    "from PIL import ImageFile\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\"\"\" data \"\"\"\n",
    "crop_size = 108\n",
    "re_size = 32\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.ToPILImage(),\n",
    "     transforms.Scale(size=(re_size, re_size), interpolation=Image.BICUBIC),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])\n",
    "\n",
    "batch_size = 16\n",
    "med_data = datasets.ImageFolder('../data_faces', transform=transform)\n",
    "data_loader = DataLoader(med_data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import os\n",
    "from itertools import chain\n",
    "from torchvision import utils\n",
    "\n",
    "SAVE_PER_TIMES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [1024, 512, 256]\n",
    "        # Input_dim = 100\n",
    "        # Output_dim = C (number of channels)\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Z latent vector 100\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (1024x4x4)\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=channels, kernel_size=4, stride=2, padding=1))\n",
    "            # output of main module --> Image (Cx32x32)\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [256, 512, 1024]\n",
    "        # Input_dim = channels (Cx64x64)\n",
    "        # Output_dim = 1\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Image (Cx32x32)\n",
    "            nn.Conv2d(in_channels=channels, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "            # output of main module --> State (1024x4x4)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            # The output of D is no longer a probability, we do not apply sigmoid at the output of D.\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def feature_extraction(self, x):\n",
    "        # Use discriminator for feature extraction then flatten to vector of 16384\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1, 1024*4*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_CP(object):\n",
    "    def __init__(self):\n",
    "        print(\"WGAN_CP init model.\")\n",
    "        channels = 3\n",
    "        self.G = Generator(channels)\n",
    "        self.D = Discriminator(channels)\n",
    "        self.C = channels\n",
    "\n",
    "        # check if cuda is available\n",
    "        self.check_cuda(True)\n",
    "\n",
    "        # WGAN values from paper\n",
    "        self.learning_rate = 0.00005\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.weight_cliping_limit = 0.01\n",
    "\n",
    "        # WGAN with gradient clipping uses RMSprop instead of ADAM\n",
    "        self.d_optimizer = torch.optim.RMSprop(self.D.parameters(), lr=self.learning_rate)\n",
    "        self.g_optimizer = torch.optim.RMSprop(self.G.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.number_of_images = 10\n",
    "\n",
    "        self.generator_iters = 10000\n",
    "        self.critic_iter = 5\n",
    "\n",
    "\n",
    "    def check_cuda(self, cuda_flag=False):\n",
    "        if cuda_flag:\n",
    "            self.cuda_index = 0\n",
    "            self.cuda = True\n",
    "            self.D.cuda()\n",
    "            self.G.cuda()\n",
    "            print(\"Cuda enabled flag: {}\".format(self.cuda))\n",
    "\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        self.t_begin = t.time()\n",
    "        #self.file = open(\"inception_score_graph.txt\", \"w\")\n",
    "\n",
    "        # Now batches are callable self.data.next()\n",
    "        self.data = self.get_infinite_batches(train_loader)\n",
    "\n",
    "        one = torch.FloatTensor([1])\n",
    "        mone = one * -1\n",
    "        if self.cuda:\n",
    "            one = one.cuda()\n",
    "            mone = mone.cuda()\n",
    "\n",
    "        for g_iter in range(self.generator_iters):\n",
    "\n",
    "            # Requires grad, Generator requires_grad = False\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # Train Dicriminator forward-loss-backward-update self.critic_iter times while 1 Generator forward-loss-backward-update\n",
    "            for d_iter in range(self.critic_iter):\n",
    "                self.D.zero_grad()\n",
    "\n",
    "                # Clamp parameters to a range [-c, c], c=self.weight_cliping_limit\n",
    "                for p in self.D.parameters():\n",
    "                    p.data.clamp_(-self.weight_cliping_limit, self.weight_cliping_limit)\n",
    "\n",
    "                images = self.data.__next__()\n",
    "                # Check for batch to have full batch_size\n",
    "                if (images.size()[0] != self.batch_size):\n",
    "                    continue\n",
    "\n",
    "                z = torch.rand((self.batch_size, 100, 1, 1))\n",
    "\n",
    "                if self.cuda:\n",
    "                    images, z = Variable(images.cuda()), Variable(z.cuda())\n",
    "                else:\n",
    "                    images, z = Variable(images), Variable(z)\n",
    "\n",
    "\n",
    "                # Train discriminator\n",
    "                # WGAN - Training discriminator more iterations than generator\n",
    "                # Train with real images\n",
    "                d_loss_real = self.D(images)\n",
    "                d_loss_real = d_loss_real.mean(0).view(1)\n",
    "                d_loss_real.backward(one)\n",
    "\n",
    "                # Train with fake images\n",
    "                if self.cuda:\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()\n",
    "                else:\n",
    "                    z = Variable(torch.randn(self.batch_size, 100, 1, 1))\n",
    "                fake_images = self.G(z)\n",
    "                d_loss_fake = self.D(fake_images)\n",
    "                d_loss_fake = d_loss_fake.mean(0).view(1)\n",
    "                d_loss_fake.backward(mone)\n",
    "\n",
    "                d_loss = d_loss_fake - d_loss_real\n",
    "                Wasserstein_D = d_loss_real - d_loss_fake\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "\n",
    "            # Generator update\n",
    "            for p in self.D.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            self.G.zero_grad()\n",
    "\n",
    "            # Train generator\n",
    "            # Compute loss with fake images\n",
    "            z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()\n",
    "            fake_images = self.G(z)\n",
    "            g_loss = self.D(fake_images)\n",
    "            g_loss = g_loss.mean().mean(0).view(1)\n",
    "            g_loss.backward(one)\n",
    "            g_cost = -g_loss\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # Saving model and sampling images every 1000th generator iterations\n",
    "            if (g_iter) % 1000 == 0:\n",
    "                self.save_model()\n",
    "                # Workaround because graphic card memory can't store more than 830 examples in memory for generating image\n",
    "                # Therefore doing loop and generating 800 examples and stacking into list of samples to get 8000 generated images\n",
    "                # This way Inception score is more correct since there are different generated examples from every class of Inception model\n",
    "                # sample_list = []\n",
    "                # for i in range(10):\n",
    "                #     z = Variable(torch.randn(800, 100, 1, 1)).cuda(self.cuda_index)\n",
    "                #     samples = self.G(z)\n",
    "                #     sample_list.append(samples.data.cpu().numpy())\n",
    "                #\n",
    "                # # Flattening list of list into one list\n",
    "                # new_sample_list = list(chain.from_iterable(sample_list))\n",
    "                # print(\"Calculating Inception Score over 8k generated images\")\n",
    "                # # Feeding list of numpy arrays\n",
    "                # inception_score = get_inception_score(new_sample_list, cuda=True, batch_size=32,\n",
    "                #                                       resize=True, splits=10)\n",
    "\n",
    "                if not os.path.exists('training_result_images/'):\n",
    "                    os.makedirs('training_result_images/')\n",
    "\n",
    "                # Denormalize images and save them in grid 8x8\n",
    "                z = Variable(torch.randn(800, 100, 1, 1)).cuda(self.cuda_index)\n",
    "                samples = self.G(z)\n",
    "                samples = samples.mul(0.5).add(0.5)\n",
    "                samples = samples.data.cpu()[:64]\n",
    "                grid = utils.make_grid(samples)\n",
    "                utils.save_image(grid, 'training_result_images/img_generatori_iter_{}.png'.format(str(g_iter).zfill(3)))\n",
    "\n",
    "                # Testing\n",
    "                time = t.time() - self.t_begin\n",
    "                #print(\"Inception score: {}\".format(inception_score))\n",
    "                print(\"Generator iter: {}\".format(g_iter))\n",
    "                print(\"Time {}\".format(time))\n",
    "\n",
    "                # Write to file inception_score, gen_iters, time\n",
    "                #output = str(g_iter) + \" \" + str(time) + \" \" + str(inception_score[0]) + \"\\n\"\n",
    "                #self.file.write(output)\n",
    "        self.t_end = t.time()\n",
    "        print('Time of training-{}'.format((self.t_end - self.t_begin)))\n",
    "        #self.file.close()\n",
    "\n",
    "        # Save the trained parameters\n",
    "        self.save_model()\n",
    "\n",
    "    def evaluate(self, test_loader, D_model_path, G_model_path):\n",
    "        self.load_model(D_model_path, G_model_path)\n",
    "        z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()\n",
    "        samples = self.G(z)\n",
    "        samples = samples.mul(0.5).add(0.5)\n",
    "        samples = samples.data.cpu()\n",
    "        grid = utils.make_grid(samples)\n",
    "        print(\"Grid of 8x8 images saved to 'dgan_model_image.png'.\")\n",
    "        utils.save_image(grid, 'dgan_model_image.png')\n",
    "\n",
    "    def real_images(self, images, number_of_images):\n",
    "        if (self.C == 3):\n",
    "            return self.to_np(images.view(-1, self.C, 32, 32)[:self.number_of_images])\n",
    "        else:\n",
    "            return self.to_np(images.view(-1, 32, 32)[:self.number_of_images])\n",
    "\n",
    "    def generate_img(self, z, number_of_images):\n",
    "        samples = self.G(z).data.cpu().numpy()[:number_of_images]\n",
    "        generated_images = []\n",
    "        for sample in samples:\n",
    "            if self.C == 3:\n",
    "                generated_images.append(sample.reshape(self.C, 32, 32))\n",
    "            else:\n",
    "                generated_images.append(sample.reshape(32, 32))\n",
    "        return generated_images\n",
    "\n",
    "    def to_np(self, x):\n",
    "        return x.data.cpu().numpy()\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.G.state_dict(), './generator.pkl')\n",
    "        torch.save(self.D.state_dict(), './discriminator.pkl')\n",
    "        print('Models save to ./generator.pkl & ./discriminator.pkl ')\n",
    "\n",
    "    def load_model(self, D_model_filename, G_model_filename):\n",
    "        D_model_path = os.path.join(os.getcwd(), D_model_filename)\n",
    "        G_model_path = os.path.join(os.getcwd(), G_model_filename)\n",
    "        self.D.load_state_dict(torch.load(D_model_path))\n",
    "        self.G.load_state_dict(torch.load(G_model_path))\n",
    "        print('Generator model loaded from {}.'.format(G_model_path))\n",
    "        print('Discriminator model loaded from {}-'.format(D_model_path))\n",
    "\n",
    "    def get_infinite_batches(self, data_loader):\n",
    "        while True:\n",
    "            for i, (images, _) in enumerate(data_loader):\n",
    "                yield images\n",
    "\n",
    "\n",
    "    def generate_latent_walk(self, number):\n",
    "        if not os.path.exists('interpolated_images/'):\n",
    "            os.makedirs('interpolated_images/')\n",
    "\n",
    "        number_int = 10\n",
    "        # interpolate between two noise (z1, z2).\n",
    "        z_intp = torch.FloatTensor(1, 100, 1, 1)\n",
    "        z1 = torch.randn(1, 100, 1, 1)\n",
    "        z2 = torch.randn(1, 100, 1, 1)\n",
    "        if self.cuda:\n",
    "            z_intp = z_intp.cuda()\n",
    "            z1 = z1.cuda()\n",
    "            z2 = z2.cuda()\n",
    "\n",
    "        z_intp = Variable(z_intp)\n",
    "        images = []\n",
    "        alpha = 1.0 / float(number_int + 1)\n",
    "        print(alpha)\n",
    "        for i in range(1, number_int + 1):\n",
    "            z_intp.data = z1*alpha + z2*(1.0 - alpha)\n",
    "            alpha += alpha\n",
    "            fake_im = self.G(z_intp)\n",
    "            fake_im = fake_im.mul(0.5).add(0.5) #denormalize\n",
    "            images.append(fake_im.view(self.C,32,32).data.cpu())\n",
    "\n",
    "        grid = utils.make_grid(images, nrow=number_int )\n",
    "        utils.save_image(grid, 'interpolated_images/interpolated_{}.png'.format(str(number).zfill(3)))\n",
    "        print(\"Saved interpolated images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WGAN_CP init model.\n",
      "Cuda enabled flag: True\n"
     ]
    }
   ],
   "source": [
    "model = WGAN_CP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 0\n",
      "Time 1.9885196685791016\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 1000\n",
      "Time 271.80806970596313\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 2000\n",
      "Time 537.7298052310944\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 3000\n",
      "Time 803.3901073932648\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 4000\n",
      "Time 1069.0664405822754\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 5000\n",
      "Time 1334.5784730911255\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 6000\n",
      "Time 1600.1878805160522\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 7000\n",
      "Time 1865.678967475891\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 8000\n",
      "Time 2134.6553163528442\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n",
      "Generator iter: 9000\n",
      "Time 2410.217955827713\n",
      "Time of training-2674.3013067245483\n",
      "Models save to ./generator.pkl & ./discriminator.pkl \n"
     ]
    }
   ],
   "source": [
    "model.train(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
